version: "3.9"
services:
  litellm:
    image: ghcr.io/berriai/litellm:main
    ports:
      - "8080:8080" # Map the container port to the host, change the host port if necessary
    volumes:
      - ./litellm-config.yaml:/app/config.yaml # Mount the local configuration file

    # You can change the port or number of workers as per your requirements or pass any new supported CLI argument. Make sure
    # the port passed here matches with the container port defined above in `ports` value
    entrypoint: ["/bin/sh", "-c", "litellm", "--config", "/app/config.yaml", "--port", "8080", "--num_workers", "8" ]
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - AZURE_API_BASE=${AZURE_API_BASE}
      - AZURE_API_VERSION=${AZURE_API_VERSION}
      - AZURE_API_KEY=${AZURE_API_KEY}
